#+BEGIN_COMMENT
.. title: Solr Note
.. slug: solr-notes
.. date: 2017-07-14 16:28:48 UTC+01:00
.. tags: solr
.. category: Notes
.. link:
.. description:
.. type: text
#+END_COMMENT

* Note on Solr Cloud

** ZooKeepers used for SolrCloud
- The main principle for a ZooKeeper ensemble is maintaining a
  majority of servers to serve request. This majority is also called a
  *quorum*.
  - If you only have two ZooKeeper nodes and one goes down, 50% of
    available servers is not a majority, so ZooKeeper will no longer
    server request.
- For a ZooKeeper service to be active, there must be a *majority* of
  non-failing machines that can communicate with each other.
- To create a deployment that can tolerate the failure of ~F~
  machines, you should count on deploying ~2xF+1~ machines.
- For this reason, ZooKeeper deployments are usually made up of an *odd*
  number of machines.
- Whereas with Solr you need to create entirely new directories to run
  multiple instances, to run a new ZooKeeper instance will only
  require a new configurationfile in
  ~<ZOOKEEPER_HOME>/conf/zoo?.cfg~. Although they will require a
  distinct ~dataDir~.
  - The configuration file is passed to ~zkServer.sh~ for start the
    instance.
- The configuration files of SolrCloud are kept in ZooKeeper.
  - They are uploaded when Solr start or collection created, or can
    be explicitly uploaded.
- To manage SolrCloud configuration files with ZooKeeper
  - Keep in mind that the configuration need to pushed t back to
    ZooKeeper when the changes.
  - And reload the collection.

** SolrCloud Concepts
- *Core* has multiple meaning:
  - In stand-alone Solr instance, a core corresponds to a
    logical/physical index.
  - In SolrCloud, a *collection* defines a logical index, where a *core*
    corresponds to a physical index residing in a node, which is a
    *portion* of the logical index.
- SolrCloud has a *leader* in every shard but the leader is largely the
  same as any other *replica*:
  - The *leader* is elected dynamically. Any replica, on any node in
    the cluster, can be appointed as the leader.
  - Both indexing documents and serving queries.
  - The only additional responsibility of the leader is to
    *distribute* documents to be /indexed to all other replicas/ in the
    shard, and to then *report* that all replicas have confirmed
    receiving a given document.
  - Any document sent into SolrCloud is /*re-routed* to the leader/ of
    the appropriate shard, who then performs this responsibility.
  - Once a document has been added to the /transaction logs/, it is
    available via a *RealTimeGet*, but is /*not available* via search
    until a soft commit or hard commit with ~openSearcher=true~/ has
    been executed.
  - When a new replica join/rejoins the cluster it will simply replays
    the transaction log to bring itself up to date with other nodes in
    the shard.
- Distributed searching is handled automatically by the nodes in the cloud.
  - Querying any node will cause that node to send the query out to
    one node in all other shards, returning a response only when it
    has aggregated the results from all shards.
  - Zookeeper and all replicas are aware of any non-responding nodes,
    and won't direct queries to nodes that are considered dead.
  - Solrj will use a simple round robin load balancer, distributing
    queries evently to all nodes in SolrCloud.
  - Solrj is Zookeeper aware and thus will never send a query to a
    node that is known as down.
- When ZooKeeper detects a leader has gone down
  - It will initiate the leader election process to select a new
    leader.
  - Transaction log ensures that all nodes in the shard are in sync,
    and all updates are durable and never lost when a leader goes
    down.
