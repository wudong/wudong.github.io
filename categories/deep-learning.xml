<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Warren Liu (Posts about deep learning)</title><link>https://wudong.graceliu.uk/</link><description></description><atom:link href="https://wudong.graceliu.uk/categories/deep-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2017 &lt;a href="mailto:wudong@graceliu.uk"&gt;Warren Liu&lt;/a&gt; </copyright><lastBuildDate>Thu, 20 Jul 2017 21:01:23 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Deep Learning in Python</title><link>https://wudong.graceliu.uk/posts/courses/deep-learning-in-python/</link><dc:creator>Warren Liu</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://wudong.graceliu.uk/posts/courses/deep-learning-in-python/#sec-1"&gt;Data Pre-processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;Data Pre-processing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Logistic regression / neural networks work on numerical vectors, not
categories.
&lt;/li&gt;
&lt;li&gt;One-hot encoding, to change a categories to  numerical values:
For example, Time of day is a category of 4: 0, 1, 2, 3, it can be
encoded as:
&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="right"&gt;

&lt;col class="right"&gt;

&lt;col class="right"&gt;

&lt;col class="right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="right"&gt;12am-6am&lt;/th&gt;
&lt;th scope="col" class="right"&gt;6am-12pm&lt;/th&gt;
&lt;th scope="col" class="right"&gt;12pm-6pm&lt;/th&gt;
&lt;th scope="col" class="right"&gt;6pm-12am&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="right"&gt;1&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;1&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;1&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;0&lt;/td&gt;
&lt;td class="right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;
Having a 1 for each column.
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Binary categories don't need 2 columns for "true" and "false". Just
absorb the "false" into the bias term.
&lt;/li&gt;
&lt;li&gt;Numerical columns: &lt;br&gt;
  Simple way to normalization (0 mean, 1 standard deviation): minus
the mean and devided by the deviation.
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>algorithm</category><category>deep learning</category><category>python</category><guid>https://wudong.graceliu.uk/posts/courses/deep-learning-in-python/</guid><pubDate>Mon, 17 Jul 2017 23:00:00 GMT</pubDate></item><item><title>Deap Learning in Udacity</title><link>https://wudong.graceliu.uk/posts/courses/deep-learning/</link><dc:creator>Warren Liu</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://wudong.graceliu.uk/posts/courses/deep-learning/#sec-1"&gt;Deep Learning in udacity.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://wudong.graceliu.uk/posts/courses/deep-learning/#sec-2"&gt;Logistic Classifier&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;Deep Learning in udacity.com&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Classification for Detection:
&lt;b&gt;To change a detection problem to a classification problem&lt;/b&gt;.
For example, to detect pedestrian from an image, use a classifier to
classify small patches of image into two classes: pedestrain and
non-pedestrain. Then run the classifier over the images's patches.
&lt;/li&gt;
&lt;li&gt;Classification for Ranking:
Use a classifier that ranks the pair (query, webpage) as input and
classify the pair as relevant/not levant. Search Engine can use the
classifier for only promising candidate webpages.
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;Logistic Classifier&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;A logistic classifier called linear classifier. It applies an linear
function to the input to generate its predictions.
&lt;/li&gt;
&lt;li&gt;Using &lt;b&gt;SOFTMAX&lt;/b&gt; function to turn the output score to probabilities, in which
the correct class will hve a probablitity close to 1 and otherwise
close to 0.
&lt;ul class="org-ul"&gt;
&lt;li&gt;Also called normalized exponential function, it "squashes" a
arbitrary k-dimensinal vector of real values to a k-dimensinal
vector of real values in the range of (0,1) that add up to 1.
&lt;/li&gt;
&lt;li&gt;The function:
\begin{equation}
\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}  \text{for } j=1..k
\end{equation}
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>deep learning</category><guid>https://wudong.graceliu.uk/posts/courses/deep-learning/</guid><pubDate>Fri, 14 Jul 2017 23:00:00 GMT</pubDate></item></channel></rss>